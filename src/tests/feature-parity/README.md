# Feature Parity Testing Framework

This framework verifies that CLIs generated by goobits maintain consistent behavior across all supported languages (Python, Node.js, TypeScript, and Rust).

## Overview

The framework uses YAML-based test definitions to specify expected behavior and automatically:
1. Generates CLIs for each language using `goobits build`
2. Executes the same tests against each implementation
3. Compares results to ensure feature parity
4. Reports any differences clearly

## Directory Structure

```
tests/feature-parity/
├── schema/
│   └── test-schema.yaml      # Test format specification
├── suites/
│   ├── basic-commands.yaml   # Basic command tests
│   ├── config-commands.yaml  # Configuration management tests
│   ├── completion-commands.yaml  # Shell completion tests
│   ├── error-handling.yaml   # Error handling consistency tests
│   └── advanced-features.yaml    # Advanced feature tests
├── runner/
│   ├── __init__.py
│   └── parity_runner.py     # Test runner implementation
├── run_parity_tests.py      # Main test execution script
├── test-cli-config.yaml     # Test CLI configuration
├── app_hooks.py             # Python hook implementations
├── hooks.js                 # Node.js/TypeScript hook implementations
└── hooks.rs                 # Rust hook implementations
```

## Running Tests

### Run all test suites:
```bash
cd tests/feature-parity
python3 run_parity_tests.py
```

### Run specific test suites:
```bash
python3 run_parity_tests.py --suite basic-commands config-commands
```

### Run with verbose output:
```bash
python3 run_parity_tests.py --verbose
```

### Test specific languages only:
```bash
python3 run_parity_tests.py --language python rust
```

## Writing Tests

### Test Suite Structure

Each test suite is a YAML file with the following structure:

```yaml
suite_name: example-suite
description: What this suite tests

setup:
  goobits_config: ../test-cli-config.yaml
  cleanup: true  # Clean up generated files after tests

tests:
  - name: test_name
    description: What this test verifies
    command: command args --options
    stdin: "optional input"
    env:
      VAR_NAME: value
    exit_code: 0
    stdout:
      contains:
        - "expected string"
      not_contains:
        - "should not appear"
      matches: "regex.*pattern"
      exact: "exact output"
    stderr:
      # Same options as stdout
    files:
      created:
        - /path/to/file
      deleted:
        - /path/to/file
      content:
        /path/to/file:
          contains:
            - "expected content"
```

### Language-Specific Behavior

When behavior differs between languages (e.g., error message formatting), use overrides:

```yaml
- name: error_test
  command: invalid
  exit_code: 2
  stderr:
    contains:
      - "error"
  language_overrides:
    nodejs:
      stderr:
        contains:
          - "error: unknown command"
    rust:
      stderr:
        contains:
          - "unrecognized subcommand"
```

### Skipping Tests

Skip tests for specific languages when the behavior cannot be unified:

```yaml
- name: interactive_test
  command: confirm
  skip_languages: ["nodejs", "typescript", "rust"]
  skip_reason: "Interactive prompts need special handling"
```

## Adding New Test Suites

1. Create a new YAML file in `suites/` directory
2. Follow the schema defined in `schema/test-schema.yaml`
3. Ensure the test CLI configuration supports your test scenarios
4. Run the tests to verify they work across all languages

## Understanding Test Results

### Success Output
```
Running test suite: basic-commands
Description: Tests for basic command execution
Tests: 10

Generating CLIs...
  python... ✓
  nodejs... ✓
  typescript... ✓
  rust... ✓

Running 10 tests...
  help_command... ✓
  version_command... ✓
  hello_basic... ✓
  ...

OVERALL: 10/10 tests passed
```

### Failure Output
```
Running 10 tests...
  hello_missing_argument... ✗
    python: Exit code mismatch: expected 2, got 1
    nodejs: stderr: Expected to contain 'required'

FAILED TESTS (1):
  basic-commands/hello_missing_argument:
    python: Exit code mismatch: expected 2, got 1
    nodejs: stderr: Expected to contain 'required'
```

## Extending the Framework

### Adding New Assertion Types

Modify `_check_output()` in `parity_runner.py` to add new assertion types:

```python
# Example: Add 'starts_with' assertion
if "starts_with" in expected:
    if not actual.startswith(expected["starts_with"]):
        errors.append(f"{output_type}: Expected to start with '{expected['starts_with']}'")
```

### Supporting New Languages

1. Add the language to `ParityTestRunner.LANGUAGES`
2. Implement CLI generation logic in `generate_cli()`
3. Add command execution logic in `run_test()`
4. Create appropriate hook file for the language

## Troubleshooting

### Tests Timeout
- Increase timeout in `run_test()` method (default: 10 seconds)
- Check for commands that wait for input

### CLI Generation Fails
- Ensure `goobits` is installed and in PATH
- Check that language-specific tools are installed (cargo, node, etc.)
- Run with `--verbose` to see detailed error messages

### Inconsistent Results
- Check hook implementations match across languages
- Verify environment differences aren't affecting tests
- Use `--verbose` to see actual command output

## Best Practices

1. **Keep tests focused**: Each test should verify one specific behavior
2. **Use descriptive names**: Test names should explain what they verify
3. **Document differences**: When behavior must differ, document why
4. **Test error cases**: Include tests for missing arguments, invalid input, etc.
5. **Consider formatting**: Don't test exact whitespace unless critical
6. **Use overrides sparingly**: Aim for consistent behavior when possible